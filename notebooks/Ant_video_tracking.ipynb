{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XMIN = 1470\n",
    "XMAX = 2840\n",
    "YMIN = 50\n",
    "YMAX = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_centroids(th, min_size):\n",
    "    if min_size < 1:\n",
    "        raise ValueError('min_size must be at least 1')\n",
    "    \n",
    "    _, cont, _ = cv2.findContours(th, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    \n",
    "    M = map(cv2.moments, cont)\n",
    "    \n",
    "    cxs = []\n",
    "    cys = []\n",
    "\n",
    "    for i, m in enumerate(M):\n",
    "        if m['m00'] < min_size:\n",
    "            continue\n",
    "\n",
    "        cx = int(m['m10']/m['m00'])\n",
    "        cy = int(m['m01']/m['m00'])\n",
    "\n",
    "        cxs.append(cx)\n",
    "        cys.append(cy)\n",
    "      \n",
    "    return cxs, cys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_masked_window(grayed, cx, cy, size):\n",
    "    windowed = grayed[cy-size//2:cy+size//2, cx-size//2:cx+size//2]\n",
    "    _, bw = cv2.threshold(windowed, 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    _, cont, _ = cv2.findContours(bw, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
    "    if 0 < len(cont) < 3:\n",
    "        return windowed * (bw == 0)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('../data/videos/TandemRun.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../data/tf_save/trained_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# CNN\n",
    "SIZE = 28\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, SIZE*SIZE])\n",
    "y_ = tf.placeholder(tf.float32, [None, 3])\n",
    "\n",
    "with tf.name_scope('reshape'):\n",
    "    x_image = tf.reshape(x, [-1, SIZE, SIZE, 1])\n",
    "    \n",
    "with tf.name_scope('conv'):\n",
    "    W_conv = weight_variable([3, 3, 1, 8])\n",
    "    b_conv = bias_variable([8])\n",
    "    h_conv = tf.nn.relu(tf.nn.conv2d(x_image, W_conv, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "    \n",
    "with tf.name_scope('pool'):\n",
    "    h_pool = tf.nn.max_pool(h_conv, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "with tf.name_scope('fc1'):\n",
    "    W_fc1 = weight_variable([SIZE//2*SIZE//2*8, 100])\n",
    "    b_fc1 = bias_variable([100])\n",
    "        \n",
    "    h_pool_flat = tf.reshape(h_pool, [-1, SIZE//2*SIZE//2*8])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool_flat, W_fc1) + b_fc1)\n",
    "    \n",
    "with tf.name_scope('fc2'):\n",
    "    W_fc2 = weight_variable([100, 3])\n",
    "    b_fc2 = bias_variable([3])\n",
    "\n",
    "    y = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "    \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "saver.restore(sess, '../data/tf_save/trained_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1s of video. Time Elapsed 7.989868879318237s\n",
      "Processed 2s of video. Time Elapsed 7.931120157241821s\n",
      "Processed 3s of video. Time Elapsed 7.93904709815979s\n",
      "Processed 4s of video. Time Elapsed 7.921381950378418s\n",
      "Processed 5s of video. Time Elapsed 7.944221019744873s\n",
      "Processed 6s of video. Time Elapsed 7.937960386276245s\n",
      "Processed 7s of video. Time Elapsed 8.165766716003418s\n",
      "Processed 8s of video. Time Elapsed 8.1509370803833s\n",
      "Processed 9s of video. Time Elapsed 8.104694843292236s\n",
      "Processed 10s of video. Time Elapsed 8.066680908203125s\n",
      "Processed 11s of video. Time Elapsed 8.124304056167603s\n"
     ]
    }
   ],
   "source": [
    "MIN_CONTOUR_SIZE = 45\n",
    "WINDOW_SIZE = SIZE\n",
    "frame_count = 0\n",
    "\n",
    "out_video = cv2.VideoWriter('../data/videos/out.mp4', cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'), 30, (3840, 2160))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "while video.isOpened():\n",
    "    ret, frame = video.read()\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % 30 == 0:\n",
    "        print('Processed {}s of video. Time Elapsed {}s'.format(frame_count//30, time.time()-start_time))\n",
    "        start_time = time.time()\n",
    "        if frame_count // 30 > 10:\n",
    "            break\n",
    "      \n",
    "    cropped = frame[YMIN:YMAX, XMIN:XMAX]\n",
    "    grayed = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
    "    th = cv2.adaptiveThreshold(grayed, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY,11,2)\n",
    "    cxs, cys = find_centroids(th, MIN_CONTOUR_SIZE)\n",
    "    \n",
    "    masks = []\n",
    "    positions = []\n",
    "    \n",
    "    for cx, cy in zip(cxs, cys):\n",
    "        masked = get_masked_window(grayed, cx, cy, WINDOW_SIZE)\n",
    "        if masked is not None and masked.size == WINDOW_SIZE * WINDOW_SIZE:\n",
    "            masks.append(masked)\n",
    "            positions.append((cx, cy))\n",
    "    # Feed masked windows to trained model for prediction.\n",
    "    predictions = sess.run(tf.argmax(y, 1), feed_dict={x: [mask.flatten() for mask in masks]})\n",
    "    for prediction, (cx, cy) in zip(predictions, positions):\n",
    "        if prediction == 1:\n",
    "            cv2.rectangle(frame, (cx+XMIN-15, cy+YMIN-15), (cx+XMIN+15, cy+YMIN+15), (0, 255, 0), 3)\n",
    "       \n",
    "    out_video.write(frame)\n",
    "\n",
    "out_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
